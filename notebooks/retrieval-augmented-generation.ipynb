{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0884c6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0212 07:48:40.691000 1968 site-packages/torch/distributed/elastic/multiprocessing/redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from pathlib import Path\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64951e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"../data/projects.jsonl\"\n",
    "EMBED_MODEL = \"intfloat/multilingual-e5-base\"  \n",
    "# Sehr gut für Deutsch + Englisch\n",
    "\n",
    "TOP_K = 5   # Wie viele Treffer pro Frage?\n",
    "\n",
    "def load_data(path):\n",
    "    documents = []\n",
    "    metadatas = []\n",
    "\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            entry = json.loads(line)\n",
    "\n",
    "            text = entry[\"text\"]\n",
    "            metadata = entry[\"metadata\"]\n",
    "\n",
    "            documents.append(text)\n",
    "            metadatas.append(metadata)\n",
    "\n",
    "    return documents, metadatas\n",
    "\n",
    "def chunk_text(text, chunk_size=400, overlap=50):\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "\n",
    "    for i in range(0, len(words), chunk_size - overlap):\n",
    "        chunk = words[i:i + chunk_size]\n",
    "        chunks.append(\" \".join(chunk))\n",
    "\n",
    "    return chunks\n",
    "\n",
    "def build_index(documents, metadatas):\n",
    "    model = SentenceTransformer(EMBED_MODEL)\n",
    "\n",
    "    all_chunks = []\n",
    "    all_metadata = []\n",
    "\n",
    "    for doc, meta in zip(documents, metadatas):\n",
    "        chunks = chunk_text(doc)\n",
    "\n",
    "        for chunk in chunks:\n",
    "            combined_text = (\n",
    "                f\"Projekt: {meta['projekt']}\\n\"\n",
    "                f\"Kategorie: {meta['kategorie']}\\n\"\n",
    "                f\"Datum: {meta['datum']}\\n\\n\"\n",
    "                f\"{chunk}\"\n",
    "            )\n",
    "            all_chunks.append(combined_text)\n",
    "            all_metadata.append(meta)\n",
    "\n",
    "    embeddings = model.encode(all_chunks, convert_to_numpy=True)\n",
    "\n",
    "    dimension = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatL2(dimension)\n",
    "    index.add(embeddings)\n",
    "\n",
    "    return model, index, all_chunks, all_metadata\n",
    "\n",
    "\n",
    "def search(query, model, index, chunks, metadata, top_k=TOP_K):\n",
    "    query_embedding = model.encode([query], convert_to_numpy=True)\n",
    "\n",
    "    distances, indices = index.search(query_embedding, top_k)\n",
    "\n",
    "    results = []\n",
    "    for idx in indices[0]:\n",
    "        results.append({\n",
    "            \"text\": chunks[idx],\n",
    "            \"metadata\": metadata[idx]\n",
    "        })\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4ef7623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lade Daten...\n",
      "aue Index...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa4fde3f9961441aac2c46ea607792ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/199 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mXLMRobertaModel LOAD REPORT\u001b[0m from: intfloat/multilingual-e5-base\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "print(\"Lade Daten...\")\n",
    "documents, metadatas = load_data(DATA_PATH)\n",
    "\n",
    "print(\"aue Index...\")\n",
    "model, index, chunks, metadata = build_index(documents, metadatas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e98dae7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_question(query, top_k=5):\n",
    "    results = search(query, model, index, chunks, metadata, top_k=top_k)\n",
    "\n",
    "    print(\"\\n Top Treffer:\\n\")\n",
    "    \n",
    "    for i, r in enumerate(results):\n",
    "        print(f\"--- Treffer {i+1} ---\")\n",
    "        print(f\"Projekt: {r['metadata']['projekt']}\")\n",
    "        print(f\"Kategorie: {r['metadata']['kategorie']}\")\n",
    "        print(f\"Datum: {r['metadata']['datum']}\")\n",
    "        print(f\"\\nText:\\n{r['text']}\\n\")\n",
    "        print(\"=\"*70)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c875a57f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Top Treffer:\n",
      "\n",
      "--- Treffer 1 ---\n",
      "Projekt: Neptune\n",
      "Kategorie: DevOps\n",
      "Datum: 2025-02-03\n",
      "\n",
      "Text:\n",
      "Projekt: Neptune\n",
      "Kategorie: DevOps\n",
      "Datum: 2025-02-03\n",
      "\n",
      "Entscheidung: Basis-Image aktualisieren und CI-Pipeline standardisieren.\n",
      "\n",
      "======================================================================\n",
      "--- Treffer 2 ---\n",
      "Projekt: Neptune\n",
      "Kategorie: Softwareentwicklung\n",
      "Datum: 2025-02-16\n",
      "\n",
      "Text:\n",
      "Projekt: Neptune\n",
      "Kategorie: Softwareentwicklung\n",
      "Datum: 2025-02-16\n",
      "\n",
      "Entwicklerperspektive: Integrationstests mussten an neue Node-Version angepasst werden.\n",
      "\n",
      "======================================================================\n",
      "--- Treffer 3 ---\n",
      "Projekt: Neptune\n",
      "Kategorie: DevOps\n",
      "Datum: 2025-02-10\n",
      "\n",
      "Text:\n",
      "Projekt: Neptune\n",
      "Kategorie: DevOps\n",
      "Datum: 2025-02-10\n",
      "\n",
      "Monitoring mit Prometheus erweitert. Neue Alerts für Memory-Leaks wurden hinzugefügt.\n",
      "\n",
      "======================================================================\n",
      "--- Treffer 4 ---\n",
      "Projekt: Neptune\n",
      "Kategorie: DevOps\n",
      "Datum: 2025-02-10\n",
      "\n",
      "Text:\n",
      "Projekt: Neptune\n",
      "Kategorie: DevOps\n",
      "Datum: 2025-02-10\n",
      "\n",
      "Monitoring-Ausbau priorisiert, um Memory-Probleme frühzeitig zu erkennen.\n",
      "\n",
      "======================================================================\n",
      "--- Treffer 5 ---\n",
      "Projekt: Neptune\n",
      "Kategorie: Support\n",
      "Datum: 2025-02-16\n",
      "\n",
      "Text:\n",
      "Projekt: Neptune\n",
      "Kategorie: Support\n",
      "Datum: 2025-02-16\n",
      "\n",
      "Support-Perspektive: Keine direkten Kundenmeldungen, Probleme sind intern begrenzt.\n",
      "\n",
      "======================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'text': 'Projekt: Neptune\\nKategorie: DevOps\\nDatum: 2025-02-03\\n\\nEntscheidung: Basis-Image aktualisieren und CI-Pipeline standardisieren.',\n",
       "  'metadata': {'projekt': 'Neptune',\n",
       "   'kategorie': 'DevOps',\n",
       "   'datum': '2025-02-03'}},\n",
       " {'text': 'Projekt: Neptune\\nKategorie: Softwareentwicklung\\nDatum: 2025-02-16\\n\\nEntwicklerperspektive: Integrationstests mussten an neue Node-Version angepasst werden.',\n",
       "  'metadata': {'projekt': 'Neptune',\n",
       "   'kategorie': 'Softwareentwicklung',\n",
       "   'datum': '2025-02-16'}},\n",
       " {'text': 'Projekt: Neptune\\nKategorie: DevOps\\nDatum: 2025-02-10\\n\\nMonitoring mit Prometheus erweitert. Neue Alerts für Memory-Leaks wurden hinzugefügt.',\n",
       "  'metadata': {'projekt': 'Neptune',\n",
       "   'kategorie': 'DevOps',\n",
       "   'datum': '2025-02-10'}},\n",
       " {'text': 'Projekt: Neptune\\nKategorie: DevOps\\nDatum: 2025-02-10\\n\\nMonitoring-Ausbau priorisiert, um Memory-Probleme frühzeitig zu erkennen.',\n",
       "  'metadata': {'projekt': 'Neptune',\n",
       "   'kategorie': 'DevOps',\n",
       "   'datum': '2025-02-10'}},\n",
       " {'text': 'Projekt: Neptune\\nKategorie: Support\\nDatum: 2025-02-16\\n\\nSupport-Perspektive: Keine direkten Kundenmeldungen, Probleme sind intern begrenzt.',\n",
       "  'metadata': {'projekt': 'Neptune',\n",
       "   'kategorie': 'Support',\n",
       "   'datum': '2025-02-16'}}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ask_question(\"Um was gehts bei Projekt Neptune?\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f5eb657",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a93a785572794963b98a964d2cd72c56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/434 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(151936, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0-35): 36 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (k_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
       "          (v_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=2048, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
       "    (rotary_emb): Qwen2RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_name = \"Qwen/Qwen2.5-3B\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "llm = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "llm = llm.to(device)\n",
    "llm.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd55f922",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def ask_question(query, top_k=5, max_new_tokens=300):\n",
    "\n",
    "    # Retrieval\n",
    "    results = search(query, model, index, chunks, metadata, top_k=top_k)\n",
    "\n",
    "    if len(results) == 0:\n",
    "        print(\"Keine relevanten Dokumente gefunden.\")\n",
    "        return\n",
    "\n",
    "    # Kontext strukturieren (Qwen mag klare Struktur)\n",
    "    context_blocks = []\n",
    "    for i, r in enumerate(results):\n",
    "        block = (\n",
    "            f\"DOKUMENT [{i+1}]\\n\"\n",
    "            f\"Projekt: {r['metadata']['projekt']}\\n\"\n",
    "            f\"Kategorie: {r['metadata']['kategorie']}\\n\"\n",
    "            f\"Datum: {r['metadata']['datum']}\\n\"\n",
    "            f\"Inhalt:\\n{r['text']}\\n\"\n",
    "        )\n",
    "        context_blocks.append(block)\n",
    "\n",
    "    full_context = \"\\n\\n\".join(context_blocks)\n",
    "\n",
    "    # Chat-Template nutzen (WICHTIG für Qwen)\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": (\n",
    "                \"Du bist ein interner Unternehmensassistent. \"\n",
    "                \"Beantworte die Frage **vollständig aus dem Kontext**. \"\n",
    "                \"Formuliere eine klare, präzise Antwort. \"\n",
    "                \"Nutze die Informationen in den Dokumenten. \"\n",
    "                \"Wenn Informationen fehlen, sage: 'Die Information ist im Kontext nicht enthalten.'\"\n",
    "            )\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Hier ist der Kontext:\\n\\n{full_context}\\n\\nFrage: {query}\\n\\nIMPORTANT: Respond in the same language as the question.\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    # Tokenisieren\n",
    "    inputs = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=2048\n",
    "    )\n",
    "\n",
    "    # ALLES explizit aufs gleiche Device wie das Modell\n",
    "    inputs = {k: v.to(llm.device) for k, v in inputs.items()}\n",
    "\n",
    "    # Generieren (deterministisch für RAG)\n",
    "    with torch.no_grad():\n",
    "        outputs = llm.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    # Nur generierten Teil dekodieren\n",
    "    generated_tokens = outputs[0][inputs[\"input_ids\"].shape[-1]:]\n",
    "    answer = tokenizer.decode(generated_tokens, skip_special_tokens=True).strip()\n",
    "\n",
    "    print(\"\\nAntwort:\\n\")\n",
    "    print(answer)\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Verwendete Quellen:\\n\")\n",
    "\n",
    "    for i, r in enumerate(results):\n",
    "        print(f\"{i+1}. {r['metadata']['projekt']} | {r['metadata']['datum']}\")\n",
    "\n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ccd033c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Antwort:\n",
      "\n",
      "Ja, es gab Probleme bei Projekt Neptune. Die Informationen in den Dokumenten zeigen, dass Probleme intern begrenzt waren, aber Integrationstests mussten an neue Node-Version angepasst werden. Darüber hinaus wurden neue Alerts für Memory-Leaks hinzugefügt, um Memory-Probleme frühzeitig zu erkennen.\n",
      "\n",
      "================================================================================\n",
      "Verwendete Quellen:\n",
      "\n",
      "1. Neptune | 2025-02-16\n",
      "2. Neptune | 2025-02-16\n",
      "3. Neptune | 2025-02-10\n",
      "4. Neptune | 2025-02-16\n",
      "5. Neptune | 2025-02-10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Ja, es gab Probleme bei Projekt Neptune. Die Informationen in den Dokumenten zeigen, dass Probleme intern begrenzt waren, aber Integrationstests mussten an neue Node-Version angepasst werden. Darüber hinaus wurden neue Alerts für Memory-Leaks hinzugefügt, um Memory-Probleme frühzeitig zu erkennen.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ask_question(\"Gab es Probleme bei Projekt Neptune?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0588fb98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Antwort:\n",
      "\n",
      "Das Projekt Helios befasst sich mit der IT-Sicherheit und beinhaltet Updates und Änderungen an TLS-Konfigurationen, Zugriffsrechten für Admin-Rollen und Code- und Config-Änderungen.\n",
      "\n",
      "================================================================================\n",
      "Verwendete Quellen:\n",
      "\n",
      "1. Helios | 2025-02-01\n",
      "2. Helios | 2025-02-08\n",
      "3. Helios | 2025-02-18\n",
      "4. Helios | 2025-03-03\n",
      "5. Helios | 2025-02-01\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Das Projekt Helios befasst sich mit der IT-Sicherheit und beinhaltet Updates und Änderungen an TLS-Konfigurationen, Zugriffsrechten für Admin-Rollen und Code- und Config-Änderungen.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ask_question(\"Worum ging es beim Projekt Helios?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33f301ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Antwort:\n",
      "\n",
      "Ja, wir haben bereits auf OAuth2 umgestellt. Die Migration auf OAuth2 ist abgeschlossen. Alte API-Keys wurden deaktiviert und die Dokumentation final aktualisiert.\n",
      "\n",
      "================================================================================\n",
      "Verwendete Quellen:\n",
      "\n",
      "1. Atlas | 2025-01-12\n",
      "2. Cipher | 2025-03-10\n",
      "3. Sentinel | 2025-03-22\n",
      "4. Sentinel | 2025-03-18\n",
      "5. Cipher | 2025-04-02\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Ja, wir haben bereits auf OAuth2 umgestellt. Die Migration auf OAuth2 ist abgeschlossen. Alte API-Keys wurden deaktiviert und die Dokumentation final aktualisiert.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ask_question(\"Haben wir bereits auf OAuth2 umgestellt? Erkläre kurz den Stand der Dinge.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "678651ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Antwort:\n",
      "\n",
      "Yes, there were problems with Project Neptune. The information in the documents indicates that there were memory issues that were not detected in time, which led to delays in feature releases. Additionally, integration tests had to be updated for a new Node version, and there were CI failures that caused further delays.\n",
      "\n",
      "================================================================================\n",
      "Verwendete Quellen:\n",
      "\n",
      "1. Neptune | 2025-02-16\n",
      "2. Neptune | 2025-02-10\n",
      "3. Neptune | 2025-02-16\n",
      "4. Neptune | 2025-02-16\n",
      "5. Neptune | 2025-02-10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Yes, there were problems with Project Neptune. The information in the documents indicates that there were memory issues that were not detected in time, which led to delays in feature releases. Additionally, integration tests had to be updated for a new Node version, and there were CI failures that caused further delays.'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ask_question(\"Were there problems with Project Neptune?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "57ae49b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Antwort:\n",
      "\n",
      "Project Helios was a project that focused on security measures despite limited resources. The project management perspective emphasized the importance of security measures, while the developer perspective required code and configuration changes to adapt to TLS configurations. The IT security perspective involved defining new admin roles, reducing rights on database and infrastructure levels, and adopting the least-privilege principle as a standard. An internal security scan identified outdated TLS configurations, and an update was planned.\n",
      "\n",
      "================================================================================\n",
      "Verwendete Quellen:\n",
      "\n",
      "1. Helios | 2025-02-18\n",
      "2. Helios | 2025-02-18\n",
      "3. Helios | 2025-02-08\n",
      "4. Helios | 2025-02-08\n",
      "5. Helios | 2025-02-01\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Project Helios was a project that focused on security measures despite limited resources. The project management perspective emphasized the importance of security measures, while the developer perspective required code and configuration changes to adapt to TLS configurations. The IT security perspective involved defining new admin roles, reducing rights on database and infrastructure levels, and adopting the least-privilege principle as a standard. An internal security scan identified outdated TLS configurations, and an update was planned.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ask_question(\"What was Project Helios about?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7fdfcbe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Antwort:\n",
      "\n",
      "Yes, we have already migrated to OAuth2. The current status is that the OAuth2 flow with Authorization Code Grant has been implemented, and the token lifetime and scope handling have been adjusted. The legacy API key validation is still active. A decision has been made to remove old API keys completely by the end of Q1 and to inform customers early. The refresh token expired server-side because the maximum validity period was exceeded. The frontend currently does not handle the error status correctly. It has been decided to extend the error handling in the frontend and to implement automatic re-authentication. The developer perspective: The migration to OAuth2 required adjustments in several services and additional tests for token flows. The DevOps perspective: Secrets management and token configuration must be centralized for OAuth2. The migration to OAuth2 is complete. Old API keys have been deactivated, and the documentation has been finalized.\n",
      "\n",
      "================================================================================\n",
      "Verwendete Quellen:\n",
      "\n",
      "1. Atlas | 2025-01-12\n",
      "2. Cipher | 2025-04-02\n",
      "3. Titan | 2025-02-15\n",
      "4. Eclipse | 2025-04-02\n",
      "5. Sentinel | 2025-03-22\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Yes, we have already migrated to OAuth2. The current status is that the OAuth2 flow with Authorization Code Grant has been implemented, and the token lifetime and scope handling have been adjusted. The legacy API key validation is still active. A decision has been made to remove old API keys completely by the end of Q1 and to inform customers early. The refresh token expired server-side because the maximum validity period was exceeded. The frontend currently does not handle the error status correctly. It has been decided to extend the error handling in the frontend and to implement automatic re-authentication. The developer perspective: The migration to OAuth2 required adjustments in several services and additional tests for token flows. The DevOps perspective: Secrets management and token configuration must be centralized for OAuth2. The migration to OAuth2 is complete. Old API keys have been deactivated, and the documentation has been finalized.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ask_question(\"Have we already migrated to OAuth2? Briefly explain the current status.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "face_craft_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
